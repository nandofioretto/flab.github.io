---
layout: default
title: Projects Summer 25
---

# Project Summer 25 - Raise Lab at UVa
(Ferdinando Fioretto)

## Constrained Generative Diffusion Processes

### Motivation
The integration of differentiable optimization techniques into diffusion models is motivated by the need to ensure that generated outputs adhere to specific properties such as physical principles, constraints, and other laws. This approach aims to create more trustworthy and reliable generative AI models.

### Research Idea
This project explores how to incorporate constraints directly into generative diffusion processes. By leveraging differentiable optimization, we can guide the generation process to produce outputs that satisfy predefined constraints, enhancing the applicability and trustworthiness of these models in various domains.

### Research Questions
- How can we integrate physical principles and other constraints into diffusion models effectively?
- What are the challenges in ensuring that the generated outputs satisfy these constraints?
- How can we quantify the reliability and trustworthiness of the constrained generative models?

### Tags
generative AI, constraints, trustworthy ML, guarantees

## Integration of ML and Optimization

### Motivation
Physics-informed machine learning has the potential to significantly accelerate optimization processes, particularly in complex systems such as energy management and scheduling. This project seeks to harness the power of ML to improve the efficiency and effectiveness of optimization tasks.

### Research Idea
By using techniques from optimization theory, we can construct surrogate models that learn to approximate optimal solutions. These models can be trained on physical principles to provide faster and more accurate optimization, reducing computational costs and improving performance.

### Research Questions
- How can physics-informed ML models be used to speed up optimization processes?
- What kind of architectures are better suited for constructing surrogate models that approximate good optimization solutions?
- How can these techniques be applied to real-world problems in energy systems and scheduling?

### Tags
differentable optimization, decision focused learning

## Integrate Constraints in LLMs

### Motivation
Providing guarantees in the outputs of large language models is crucial for ensuring their reliability and trustworthiness. Constraints help in formalizing these guarantees, making the models more robust and dependable.

### Research Idea
This project aims to formalize and impose constraints on the outputs of autoregressive generative models. By defining and integrating constraints, we can guide the generation process to produce outputs that meet specific criteria, enhancing their utility and trustworthiness.

### Research Questions
- How do we formalize constraints in the context of LLMs?
- What methods can be used to impose these constraints effectively?
- How do constraints impact the performance and reliability of LLM outputs?

### Tags
Genrative AI, LLMs, guarantees, trustworthy ML

## Data Minimization

### Motivation
The principle of data minimization is central to data privacy, aiming to collect and process only the data necessary for a specific purpose. However, its application in machine learning systems requires careful definition and assessment.

### Research Idea
This project seeks to define what data minimization means for ML systems and how to assess its privacy goals. It includes developing notions of data minimization that have legal validity, are useful for policy makers, and can be implemented efficiently in ML algorithms.

### Research Questions
- How should data minimization be defined in the context of ML systems?
- What metrics can be used to assess the privacy goals of data minimization?
- How can data minimization principles be integrated into ML algorithms effectively?

### Tags
data privacy, guarantees, privacy-preserving ML

## Fairness in ML

### Motivation
Fairness in machine learning (ML) is crucial to ensure that ML systems do not perpetuate or exacerbate biases. This is especially important when these systems are constrained by privacy, space, and robustness requirements, which can lead to unintended fairness issues.

### Research Idea
This project aims to explore how constraints in ML systems impact fairness. By studying the effects of privacy, space, and robustness constraints, we can develop methods to mitigate unintended consequences and improve fairness.

### Research Questions
- How do privacy, space, and robustness constraints affect fairness in ML systems?
- What are the unintended consequences of these constraints on fairness?
- How can we design ML systems that balance these constraints while maintaining fairness?

### Tags
Fairness, Privacy 

## Fairness in LLMs

### Motivation
Fairness in large language models (LLMs) is essential to avoid generating biased or harmful content. Unfairness can arise in various stages of model training and deployment, making it critical to understand and address these issues.

### Research Idea
This project focuses on studying fairness in LLMs, particularly in techniques like Low-Rank Adaptation (LoRA). By examining how unfairness arises and identifying mitigation strategies, we aim to improve the fairness of LLM outputs.

### Research Questions
- What are the key factors that contribute to unfairness in LLMs?
- How does LoRA impact fairness in LLMs?
- What methods can be used to mitigate unfairness in LLMs?

### Tags
Fairness, LLMs 

## Unlearning

### Motivation
Unlearning in machine learning (ML) refers to the ability to remove specific data points or information from a model, ensuring that the model no longer retains any influence from the removed data. This is important for privacy, compliance, and correcting errors.

### Research Idea
This project explores methods to provide unlearning guarantees, develop unlearning concepts, and implement unlearning under various constraints. It also investigates unlearning in the context of LLMs to ensure that these models can forget specific information as needed.

### Research Questions
- How can we provide formal guarantees for unlearning in ML models?
- What are the key concepts and techniques for effective unlearning?
- How can unlearning be implemented in LLMs while maintaining model performance?

### Tags
Privacy preserving ML, LLMs, differntial privacy

## Privacy and Fairness

### Motivation
Differential privacy (DP) is a technique used to protect individual data points in a dataset. However, it can also affect the fairness of machine learning models, potentially introducing biases or impacting model performance.

### Research Idea
This project investigates the theoretical analysis of the bias and variance introduced by differential privacy mechanisms. It focuses on specific applications, such as resource allocation, to understand the trade-offs between privacy and fairness.

### Research Questions
- How does differential privacy impact the fairness of ML models?
- What are the theoretical implications of DP mechanisms on bias and variance?
- How can we balance privacy and fairness in specific applications like resource allocation?

### Tags
Privacy preserving ML, differntial privacy, fairness